# Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems

## 1 Introduction

Deep recommendation models:

- data hungry, likely to overfit without enough data
- bottleneck: data sparsity

prompt learning:

- rely on a suite of appropriate prompts
- reformulate the downstream tasks as the pre-training task

## 2 Generic Architecture of LMRS

## 3 Data Types

- Textual
- Sequential data: User-item interactions
- Graphs
- Multi-modal data

## 4 LMRS Training Strategies

### 4.1 Pre-train, fine-tune paradigm

Advantages:

- better generalization on different downstream recommendation tasks
- universal knowledge
- can avoid overfitting

#### Pre-train

Traditional end-to-end

#### Pre-train, fine-tune holistic model

fine-tune adjust the whole model parameters

#### Pre-train, fine-tune partial model

fine-tune partial parameters

#### Pre-train, fine-tune extra part of the model

fine-tune the extra parts



### 4.2 Prompting paradigm

#### Fixed-PTM prompt tuning

- Choose prompt carefully

Prompts:

- Discrete textual template: human readable
- soft continuous vectors: generated by a net according to the user information：embeddings

#### Fixed-prompt PTM tuning

The fixed prompts are related to the tasks

pretrain and fine-tune

#### Tuning-free prompting

zero-shot recommendations

no fine-tuning

#### Prompt+PTM tuning

pretrain and fine-tune all parameters

in addition to the prompt tuning

提示学习 https://zhuanlan.zhihu.com/p/524383554



## 5 Training Objectives

### 5.1 Language modelling objects 

#### Partial/ Auto-regressive modelling 

单向语言模型，使用对数loss

GPT-2 DialoGPT

依赖一侧的输入，用于生成模型，下一个token的输入为前面生成的tokens

#### MLM

mask some tokens in the text as the input 

use the other tokens to predict the lost ones

#### Next Sentence Prediction (NSP)

to predict whether the two given sentences follow 

- SOP: Sentence Order Prediction, the order is right or wrong

#### Replaced Token Detection (RTD)

whether a token is replaced



### 5.2 Adaptive objectives to recommendation

- Auto-regressive / MLM: News clicked history
- Rearrange Sequence Prediction: Whether the user interaction history has been rearranged
- MLM -> learn graph representations ->  Masked Node Prediction, Masked Edge Prediction
- NSP -> CTR, Next K Behaviors 



## 6 